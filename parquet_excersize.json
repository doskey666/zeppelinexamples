{"paragraphs":[{"title":"Write benchmarks","text":"\ndef write(name: String, dat: org.apache.spark.sql.DataFrame) {\n    \n    import com.databricks.spark.avro._\n\n    dat.cache\n    var num_records = dat.count\n    var t0 = System.currentTimeMillis()\n    dat.write.mode(\"overwrite\").json(\"/tmp/\" + name + \"_json\")\n    println(\"Writing \" + name + \" in json elapsed time: \" + (System.currentTimeMillis() - t0) + \"ms \" + num_records + \" records\")\n    \n    t0 = System.currentTimeMillis()\n    dat.write.mode(\"overwrite\").parquet(\"/tmp/\" + name + \"_parquet\")\n    println(\"Writing \" + name + \" in parquet elapsed time: \" + (System.currentTimeMillis() - t0) + \"ms \" + num_records + \" records\")\n    \n    spark.conf.set(\"spark.sql.avro.compression.codec\", \"snappy\")\n\n    t0 = System.currentTimeMillis()\n    dat.write.mode(\"overwrite\").avro(\"/tmp/\" + name + \"_avro\")\n    println(\"Writing \" + name + \" in avro elapsed time: \" + (System.currentTimeMillis() - t0) + \"ms \" + num_records + \" records\")\n\n    dat.unpersist\n}\n\nimport spark.implicits._\n\n//  Generate 10 mil running numbers\nvar num_of_records = 10000000\nvar random_range = 100000\nvar range_of_numbers = sqlContext.range(0, num_of_records).repartition(1)\ncase class Record(id: Long, rand_uniform: Long, string_value: String)\n\n// Add two columns, one with random uniform distribution and another with 64 characters random string\nvar random_numbers_base = range_of_numbers.select(range_of_numbers(\"id\"), rand(seed=666).alias(\"uniform\"))\nvar rnd = new scala.util.Random()\nvar random_numbers = random_numbers_base.map( x=> Record(x.getAs[Long](\"id\"), \n                                                         Math.round(x.getAs[Double](\"uniform\") * random_range),\n                                                         rnd.nextString(64)))\n\nwrite(\"running_ids\", random_numbers.select(\"id\")) // This stores only the id column\nwrite(\"running_ids_very_long_name\", random_numbers.select(\"id\").toDF(\"very_long_name_for_id_column\")) // This stores only the id column using a very long column name\nwrite(\"rand_uniform\", random_numbers.select(\"rand_uniform\")) // This stores only the random column\nwrite(\"sorted_rand_uniform\", random_numbers.select(\"rand_uniform\").sort(\"rand_uniform\")) // This stores only the random column but sorted\nwrite(\"all_values\", random_numbers.toDF) // This stores all of the data frame (id, rand_uniform, string_value), if you get out of memory exception make sure to add into conf/zeppelin-env.sh the value 'export ZEPPELIN_INTP_MEM=\"-Xmx3072m -Xms3072m -XX:MaxPermSize=512m\"'\n\n\n","dateUpdated":"2016-12-11T03:13:52+0200","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"title":true,"editorMode":"ace/mode/scala","editorHide":false},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1481104286666_-851554141","id":"20161207-115126_1502642680","result":{"code":"SUCCESS","type":"TEXT","msg":"\nwrite: (name: String, dat: org.apache.spark.sql.DataFrame)Unit\n\nimport spark.implicits._\n\nnum_of_records: Int = 10000000\n\nrandom_range: Int = 100000\n\nrange_of_numbers: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [id: bigint]\n\ndefined class Record\n\nrandom_numbers_base: org.apache.spark.sql.DataFrame = [id: bigint, uniform: double]\n\nrnd: scala.util.Random = scala.util.Random@4ecccb24\n\nrandom_numbers: org.apache.spark.sql.Dataset[Record] = [id: bigint, rand_uniform: bigint ... 1 more field]\nWriting running_ids in json elapsed time: 2863ms 10000000 records\nWriting running_ids in parquet elapsed time: 1330ms 10000000 records\nWriting running_ids in avro elapsed time: 4473ms 10000000 records\nWriting running_ids_very_long_name in json elapsed time: 4145ms 10000000 records\nWriting running_ids_very_long_name in parquet elapsed time: 1432ms 10000000 records\nWriting running_ids_very_long_name in avro elapsed time: 4436ms 10000000 records\nWriting rand_uniform in json elapsed time: 3183ms 10000000 records\nWriting rand_uniform in parquet elapsed time: 1762ms 10000000 records\nWriting rand_uniform in avro elapsed time: 4228ms 10000000 records\nWriting sorted_rand_uniform in json elapsed time: 1822ms 10000000 records\nWriting sorted_rand_uniform in parquet elapsed time: 1180ms 10000000 records\nWriting sorted_rand_uniform in avro elapsed time: 1735ms 10000000 records\nWriting all_values in json elapsed time: 22174ms 10000000 records\nWriting all_values in parquet elapsed time: 14367ms 10000000 records\nWriting all_values in avro elapsed time: 25220ms 10000000 records\n"},"dateCreated":"2016-12-07T11:51:26+0200","dateStarted":"2016-12-11T03:07:00+0200","dateFinished":"2016-12-11T03:12:31+0200","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:88","focus":true},{"text":"%sh\n\ndu -h -d1 /tmp/ | grep 'avro\\|json\\|parquet'","dateUpdated":"2016-12-11T03:13:20+0200","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/sh","title":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1481119657138_1406381891","id":"20161207-160737_558218118","result":{"code":"SUCCESS","type":"TEXT","msg":"1.9G\t/tmp//all_values_avro\n2.3G\t/tmp//all_values_json\n1.9G\t/tmp//all_values_parquet\n 38M\t/tmp//rand_uniform_avro\n220M\t/tmp//rand_uniform_json\n 21M\t/tmp//rand_uniform_parquet\n 38M\t/tmp//running_ids_avro\n143M\t/tmp//running_ids_json\n 38M\t/tmp//running_ids_parquet\n 38M\t/tmp//running_ids_very_long_name_avro\n393M\t/tmp//running_ids_very_long_name_json\n 38M\t/tmp//running_ids_very_long_name_parquet\n3.5M\t/tmp//sorted_rand_uniform_avro\n221M\t/tmp//sorted_rand_uniform_json\n2.1M\t/tmp//sorted_rand_uniform_parquet\n"},"dateCreated":"2016-12-07T04:07:37+0200","dateStarted":"2016-12-11T03:13:20+0200","dateFinished":"2016-12-11T03:13:20+0200","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:89","title":"Check file sizes","focus":true},{"text":"\ndef countRecords(name: String, dat: org.apache.spark.sql.DataFrame) {\n    \n    var t0 = System.currentTimeMillis()\n    dat.createOrReplaceTempView(name)\n    var v = sqlContext.sql(\"select count(*) from \" + name).collect\n    println(\"Counting \" + name + \", elapsed time: \" + (System.currentTimeMillis() - t0) + \"ms \" + v.head.get(0))\n\n}\n\ndef sumValues(name: String, dat: org.apache.spark.sql.DataFrame) {\n    \n    var t0 = System.currentTimeMillis()\n    dat.createOrReplaceTempView(name)\n    var v = sqlContext.sql(\"select sum(rand_uniform) from \" + name).collect\n    println(\"Counting \" + name + \", elapsed time: \" + (System.currentTimeMillis() - t0) + \"ms \" + v.head.get(0))\n\n}\n\nimport com.databricks.spark.avro._\n\ncountRecords(\"all_values_json\", sqlContext.read.json(\"/tmp/all_values_json\"))\ncountRecords(\"all_values_parquet\", sqlContext.read.parquet(\"/tmp/all_values_parquet\"))\ncountRecords(\"all_values_avro\", sqlContext.read.avro(\"/tmp/all_values_avro\"))\n\nsumValues(\"all_values_json\", sqlContext.read.json(\"/tmp/all_values_json\"))\nsumValues(\"all_values_parquet\", sqlContext.read.parquet(\"/tmp/all_values_parquet\"))\nsumValues(\"all_values_avro\", sqlContext.read.avro(\"/tmp/all_values_avro\"))\n","dateUpdated":"2016-12-11T03:23:05+0200","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala","title":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1481416165493_212641393","id":"20161211-022925_847537919","dateCreated":"2016-12-11T02:29:25+0200","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:90","dateFinished":"2016-12-11T03:23:31+0200","dateStarted":"2016-12-11T03:23:05+0200","title":"Read benchmarks","result":{"code":"SUCCESS","type":"TEXT","msg":"\ncountRecords: (name: String, dat: org.apache.spark.sql.DataFrame)Unit\n\nsumValues: (name: String, dat: org.apache.spark.sql.DataFrame)Unit\n\nimport com.databricks.spark.avro._\nCounting all_values_json, elapsed time: 5498ms 10000000\nCounting all_values_parquet, elapsed time: 50ms 10000000\nCounting all_values_avro, elapsed time: 1602ms 10000000\nCounting all_values_json, elapsed time: 5901ms 499964480723\nCounting all_values_parquet, elapsed time: 115ms 499964480723\nCounting all_values_avro, elapsed time: 1737ms 499964480723\n"},"focus":true},{"config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1481419355513_1302798659","id":"20161211-032235_1982279757","dateCreated":"2016-12-11T03:22:35+0200","status":"READY","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:838","dateUpdated":"2016-12-11T03:23:14+0200","text":""}],"name":"parquet_excersize","id":"2C4YCKE9P","angularObjects":{"2C5WQJ9XA:shared_process":[],"2C3WU3777:shared_process":[],"2C5KN29NS:shared_process":[],"2C3ZGWRRG:shared_process":[],"2C5FE5V4C:shared_process":[],"2C4GTMJAU:shared_process":[],"2C4BY4KUJ:shared_process":[],"2C2JARAY3:shared_process":[],"2C5NK361F:shared_process":[],"2C5YT22EF:shared_process":[],"2C4F2P8PA:shared_process":[],"2C4PD5RR9:shared_process":[],"2C54P6UJ7:shared_process":[],"2C2M3VHY8:shared_process":[],"2C5EV9N12:shared_process":[],"2C4UPP9G3:shared_process":[],"2C3AU55GD:shared_process":[],"2C39P8TRG:shared_process":[]},"config":{"looknfeel":"default"},"info":{}}
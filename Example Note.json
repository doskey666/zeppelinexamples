{"paragraphs":[{"title":"Dependencies","text":"%dep\nz.reset()\nz.addRepo(\"Spark Packages Repo\").url(\"http://dl.bintray.com/spark-packages/maven\")\nz.addRepo(\"spray repo\").url(\"http://repo.spray.io\")\n\nz.load(\"com.databricks:spark-csv_2.11:1.3.0\")\n// z.load(\"com.databricks:spark-xml_2.11:0.3.3\")\n// z.load(\"org.apache.hadoop:hadoop-aws:2.6.0\")\n// z.load(\"io.spray:spray-json_2.10:1.3.2\")\n// z.load(\"org.scala-lang:scala-actors:2.10.0-M6\")\n// z.load(\"io.spray:spray-can:1.3.1\")\n\n// z.load(\"net.databinder.dispatch:dispatch-core:0.11.2\")\n// com.amazonaws:aws-java-sdk-s3:1.10.38\n// com.amazonaws:aws-java-sdk-core:1.10.38\n","dateUpdated":"Jul 4, 2016 7:30:22 PM","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala","title":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1467659815120_-88185209","id":"20160704-191655_1488499392","dateCreated":"Jul 4, 2016 7:16:55 PM","status":"READY","progressUpdateIntervalMs":500,"$$hashKey":"object:1422"},{"title":"UDFS","text":"// Http get call\ndef getUrlAsString(url: String): String = {\n  var inputUrl = url\n  val client = new org.apache.http.impl.client.DefaultHttpClient()\n  val request = new org.apache.http.client.methods.HttpGet(inputUrl)\n  val response = client.execute(request)\n  val handler = new org.apache.http.impl.client.BasicResponseHandler()\n  handler.handleResponse(response).trim\n}\nsqlContext.udf.register(\"getUrlAsString\", (url: String) => getUrlAsString(url))\n\n// // Http post call\ndef getUrlAsStringWithBody(url: String, body: String): String = {\n  val client = new org.apache.http.impl.client.DefaultHttpClient()\n  val request = new org.apache.http.client.methods.HttpPost(url)\n  val entity = new org.apache.http.entity.StringEntity(body, \"utf-8\")\n  request.setEntity(entity)\n  request.addHeader(\"Content-Type\", \"application/json; charset=utf-8\")\n\n  val response = client.execute(request)\n  val handler = new org.apache.http.impl.client.BasicResponseHandler()\n  handler.handleResponse(response).trim\n}\nsqlContext.udf.register(\"getUrlAsStringWithBody\", (url: String, body: String) => getUrlAsStringWithBody(url, body))\n\n// Extract date (regexp pattern example)\ndef extractDate(str: String): String = {\n    if (Option(str).getOrElse(\"\").isEmpty) {\n        return null\n    }\n    \n    val pattern = \"\"\".*(\\d{1,2})[\\.\\-\\\\\\/]{1}(\\d{1,2})[\\.\\-\\\\\\/]{1}(\\d{4}).*\"\"\".r\n    try {\n        val pattern(firstStr, secondStr, year) = str\n        val first = Integer.valueOf(firstStr)\n        val second = Integer.valueOf(secondStr)\n        val secondGreaterThan12 = second > 12\n        val firstGreaterThan12 = first > 12\n        if (secondGreaterThan12 && firstGreaterThan12) {\n            return null\n        }\n        \n        if (secondGreaterThan12) {\n            return year + \"-\" + \"%02d\".format(first) + \"-\" + \"%02d\".format(second)\n        } else if (firstGreaterThan12) {\n            return year + \"-\" + \"%02d\".format(second) + \"-\" + \"%02d\".format(first)\n        } else { // assume day/month/year\n            return year + \"-\" + \"%02d\".format(second) + \"-\" + \"%02d\".format(first)\n        }\n        \n        return null\n    } catch {\n     case matchException: MatchError => return null\n    }\n    \n    return null\n}\nsqlContext.udf.register(\"extractDate\", (str: String) => extractDate(str))\n// println(extractDate(\"1/20/2019 sadasd\"))\n// println(extractDate(\"20/01/2019 sadasd\"))\n// println(extractDate(\"20/2/2019 sadasd\"))\n\n// Read Json To Table\nimport java.io.File\ndef readJsonFile(fileFullPath: String, schemaName: String, tableName: String, dropIfExists: Boolean) {\n    var jsonFile = sqlContext.jsonFile(fileFullPath) // eg \"/data/blabla.json\" - files should be one json per line\n    jsonFile.registerTempTable(tableName + \"_temp\")\n    \n    sqlContext.sql(\"CREATE SCHEMA IF NOT EXISTS \" + schemaName)\n\n    if (dropIfExists) {\n      sqlContext.sql(\"DROP TABLE IF EXISTS \" + schemaName + \".\" + tableName)\n      sqlContext.sql(\"DROP TABLE IF EXISTS \" + schemaName + \".\" + tableName + \"_prqt\")\n    }\n\n    println(\"Creating table \" + tableName)\n    sqlContext.sql(\"CREATE TABLE \" + schemaName + \".\" + tableName + \" stored as sequencefile as select * from \" + tableName + \"_temp\")\n\n    println(\"Creating table \" + tableName + \"_prqt\")\n    sqlContext.sql(\"CREATE TABLE \" + schemaName + \".\" + tableName + \"_prqt stored as parquet as select * from \" + tableName + \"_temp\")\n\n    sqlContext.dropTempTable(tableName + \"_temp\")\n}\n\n// Read csv to table\ndef getCsvIntoTable(fileFullPath: String, schemaName: String, tableName: String, dropIfExists: Boolean) {\n\n      var csvFile = sqlContext.read\n            .format(\"com.databricks.spark.csv\")\n            .option(\"header\", \"true\") // Use first line of all files as header\n            .option(\"inferSchema\", \"true\") // Automatically infer data types\n            .load(fileFullPath)\n      csvFile.registerTempTable(tableName + \"_temp\")\n\n      sqlContext.sql(\"CREATE SCHEMA IF NOT EXISTS \" + schemaName)\n      \n      if (dropIfExists) {\n          sqlContext.sql(\"DROP TABLE IF EXISTS \" + schemaName + \".\" + tableName)\n          sqlContext.sql(\"DROP TABLE IF EXISTS \" + schemaName + \".\" + tableName + \"_prqt\")\n      }\n      \n      println(\"Creating table \" + tableName)\n      sqlContext.sql(\"CREATE TABLE \" + schemaName + \".\" + tableName + \" stored as sequencefile as select * from \" + tableName + \"_temp\")\n      \n      println(\"Creating table \" + tableName + \"_prqt\")\n      sqlContext.sql(\"CREATE TABLE \" + schemaName + \".\" + tableName + \"_prqt stored as parquet as select * from \" + tableName + \"_temp\")\n      \n      sqlContext.dropTempTable(tableName + \"_temp\")\n}\n","dateUpdated":"Jul 30, 2016 6:56:07 PM","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala","title":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1467659839841_-923444422","id":"20160704-191719_1817563478","dateCreated":"Jul 4, 2016 7:17:19 PM","status":"READY","progressUpdateIntervalMs":500,"$$hashKey":"object:1423"},{"text":"%sh\n\nexport INDICES=\"\"\n# export INDICES=$INDICES\" table_name1,index_name1/type_name1\"\n# export INDICES=$INDICES\" table_name2,index_name2/type_name2\"\n\nexport ELASTICSEARCH_URL=\"https://user:pass@host:port\"\nexport OUTPUT_DIR=\"/data/datafiles/es\"\n\ntouch $OUTPUT_DIR/working\n\nfor i in `echo $INDICES`; do\n  fileName=$(echo $i | cut -f1 -d,)\n  index=$(echo $i | cut -f2 -d,)\n  sourceOnly=$(echo $i | cut -f3 -d,)\n  if [ -z $sourceOnly ]\n  then\n    sourceOnly=\"--sourceOnly\"\n  else\n    sourceOnly=\"\"\n  fi;\n  echo \"fileName=\"$fileName | tee -a $OUTPUT_DIR/export.log\n  echo \"index=\"$index | tee -a $OUTPUT_DIR/export.log\n  echo \"sourceOnly=\"$sourceOnly | tee -a $OUTPUT_DIR/export.log\n\n  rm $OUTPUT_DIR/$fileName.json\n  outputFile=$OUTPUT_DIR/$fileName.json\n\n  touch $outputFile.writing\n  NODE_TLS_REJECT_UNAUTHORIZED=0 elasticdump --input=$ELASTICSEARCH_URL--input-index=$index --output=$outputFile --type=data $sourceOnly --limit 1000 --maxSockets 1 | tee -a $OUTPUT_DIR/export.log | tail\n  rm $outputFile.writing\n  touch $outputFile.unread\n  \ndone;\n\ncat $OUTPUT_DIR/questionaire_results.json | sed 's/\\\\n/ /g' > $OUTPUT_DIR/questionaire_results_no_newline.json\ntouch $$OUTPUT_DIR/questionaire_results_no_newline.json.unread\n\nrm $OUTPUT_DIR/working","dateUpdated":"Jul 30, 2016 6:53:00 PM","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/sh","title":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1467660044546_1922449602","id":"20160704-192044_2012549397","dateCreated":"Jul 4, 2016 7:20:44 PM","status":"READY","progressUpdateIntervalMs":500,"$$hashKey":"object:1424","title":"Dump elasticsearch"},{"config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala","title":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1469904819034_-1465284188","id":"20160730-185339_1776246656","dateCreated":"Jul 30, 2016 6:53:39 PM","status":"READY","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:1475","text":"import java.io.File\n\n\nsqlContext.sql(\"CREATE SCHEMA IF NOT EXISTS es\")\nval esDataDir = \"/data/datafiles/es\"\nval schema = \"es\"\nsqlContext.sql(\"CREATE SCHEMA IF NOT EXISTS \" + schema)\n\nwhile (new File(esDataDir + \"/working\").exists) { // This is to synchronize the paragraph\n  Thread.sleep(1000)\n}\n\nval unredFiles = new File(esDataDir).listFiles.filter(_.isFile).filter(_.toString().endsWith(\".unread\")).toList\nval jsonFiles = unredFiles.map(_.getName.replaceAll(\"\\\\.json\\\\.unread$\", \"\"));\n\nfor (currentJsonFileName <- jsonFiles) {\n    println(currentJsonFileName)\n\n    var jsonFile = sqlContext.jsonFile(esDataDir + \"/\" + currentJsonFileName + \".json\")\n    jsonFile.registerTempTable(currentJsonFileName + \"_temp\")\n\n    println(\"Creating table \" + currentJsonFileName)\n    var storageClause = \"stored as sequencefile\"\n    // if (currentJsonFileName.equals(\"table_with_lots_of_nested_fields\")) {\n    //     storageClause = \"ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe' WITH SERDEPROPERTIES (  'hive.serialization.extend.nesting.levels'='true')\"\n    // }\n    sqlContext.sql(\"DROP TABLE IF EXISTS \" schema + \".\" + currentJsonFileName);\n    sqlContext.sql(\"CREATE TABLE IF NOT EXISTS \" schema + \".\" + currentJsonFileName + \" \" + storageClause + \" as select * from \" + currentJsonFileName + \"_temp\")\n\n    println(\"Creating table \" + currentJsonFileName + \"_prqt\")\n    sqlContext.sql(\"DROP TABLE IF EXISTS \" schema + \".\" + currentJsonFileName + \"_prqt\");\n    sqlContext.sql(\"CREATE TABLE IF NOT EXISTS \" schema + \".\" + currentJsonFileName + \"_prqt stored as parquet as select * from \" schema + \".\" + currentJsonFileName)\n\n    sqlContext.dropTempTable(currentJsonFileName + \"_temp\")\n    \n    new File(esDataDir + \"/\" + currentJsonFileName + \".json.unread\").delete\n}\n\nnew File(esDataDir + \"/working\").delete","dateUpdated":"Jul 30, 2016 6:59:00 PM","title":"Load elasticsearch dump to Spark"}],"name":"Example Note","id":"2BS7MNSJE","angularObjects":{"2BE97ZHKT":[],"2BFTPSDFF":[],"2BH8BP239":[],"2BDMAYNUU":[],"2BHHPN55J":[],"2BHHPDDZ2":[],"2BEEP7TWJ":[],"2BF5DXJEF":[],"2BFQRNZ7G":[],"2BGGAQ3X9":[],"2BG5VH99W":[],"2BH7XF1R9":[],"2BFP3A4ZT":[],"2BDZ52U37":[]},"config":{"looknfeel":"default"},"info":{}}
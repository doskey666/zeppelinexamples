{"paragraphs":[{"title":"Dependencies","dateUpdated":"Jul 4, 2016 7:17:28 PM","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala","title":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1467659815120_-88185209","id":"20160704-191655_1488499392","dateCreated":"Jul 4, 2016 7:16:55 PM","status":"READY","progressUpdateIntervalMs":500,"$$hashKey":"object:403","text":"%dep\nz.reset()\nz.addRepo(\"Spark Packages Repo\").url(\"http://dl.bintray.com/spark-packages/maven\")\nz.addRepo(\"spray repo\").url(\"http://repo.spray.io\")\n\nz.load(\"com.databricks:spark-csv_2.11:1.3.0\")\n// z.load(\"com.databricks:spark-xml_2.11:0.3.3\")\n// z.load(\"org.apache.hadoop:hadoop-aws:2.6.0\")\n// z.load(\"io.spray:spray-json_2.10:1.3.2\")\n// z.load(\"org.scala-lang:scala-actors:2.10.0-M6\")\n// z.load(\"io.spray:spray-can:1.3.1\")\n\n// z.load(\"net.databinder.dispatch:dispatch-core:0.11.2\")\n// com.amazonaws:aws-java-sdk-s3:1.10.38\n// com.amazonaws:aws-java-sdk-core:1.10.38\n"},{"config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala","title":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1467659839841_-923444422","id":"20160704-191719_1817563478","dateCreated":"Jul 4, 2016 7:17:19 PM","status":"READY","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:422","dateUpdated":"Jul 4, 2016 7:29:52 PM","title":"UDFS","text":"// Http get call\ndef getUrlAsString(url: String): String = {\n  var inputUrl = url\n  val client = new org.apache.http.impl.client.DefaultHttpClient()\n  val request = new org.apache.http.client.methods.HttpGet(inputUrl)\n  val response = client.execute(request)\n  val handler = new org.apache.http.impl.client.BasicResponseHandler()\n  handler.handleResponse(response).trim\n}\nsqlContext.udf.register(\"getUrlAsString\", (url: String) => getUrlAsString(url))\n\n// // Http post call\ndef getUrlAsStringWithBody(url: String, body: String): String = {\n  val client = new org.apache.http.impl.client.DefaultHttpClient()\n  val request = new org.apache.http.client.methods.HttpPost(url)\n  val entity = new org.apache.http.entity.StringEntity(body, \"utf-8\")\n  request.setEntity(entity)\n  request.addHeader(\"Content-Type\", \"application/json; charset=utf-8\")\n\n  val response = client.execute(request)\n  val handler = new org.apache.http.impl.client.BasicResponseHandler()\n  handler.handleResponse(response).trim\n}\nsqlContext.udf.register(\"getUrlAsStringWithBody\", (url: String, body: String) => getUrlAsStringWithBody(url, body))\n\n// Extract date (regexp pattern example)\ndef extractDate(str: String): String = {\n    if (Option(str).getOrElse(\"\").isEmpty) {\n        return null\n    }\n    \n    val pattern = \"\"\".*(\\d{1,2})[\\.\\-\\\\\\/]{1}(\\d{1,2})[\\.\\-\\\\\\/]{1}(\\d{4}).*\"\"\".r\n    try {\n        val pattern(firstStr, secondStr, year) = str\n        val first = Integer.valueOf(firstStr)\n        val second = Integer.valueOf(secondStr)\n        val secondGreaterThan12 = second > 12\n        val firstGreaterThan12 = first > 12\n        if (secondGreaterThan12 && firstGreaterThan12) {\n            return null\n        }\n        \n        if (secondGreaterThan12) {\n            return year + \"-\" + \"%02d\".format(first) + \"-\" + \"%02d\".format(second)\n        } else if (firstGreaterThan12) {\n            return year + \"-\" + \"%02d\".format(second) + \"-\" + \"%02d\".format(first)\n        } else { // assume day/month/year\n            return year + \"-\" + \"%02d\".format(second) + \"-\" + \"%02d\".format(first)\n        }\n        \n        return null\n    } catch {\n     case matchException: MatchError => return null\n    }\n    \n    return null\n}\nsqlContext.udf.register(\"extractDate\", (str: String) => extractDate(str))\n// println(extractDate(\"1/20/2019 sadasd\"))\n// println(extractDate(\"20/01/2019 sadasd\"))\n// println(extractDate(\"20/2/2019 sadasd\"))\n\n// Read Json To Table\nimport java.io.File\ndef readJsonFile(fileFullPath: String, schemaName: String, tableName: String, dropIfExists: Boolean) {\n    var jsonFile = sqlContext.jsonFile(fileFullPath) // eg \"/data/blabla.json\" - files should be one json per line\n    jsonFile.registerTempTable(tableName + \"_temp\")\n    \n    sqlContext.sql(\"CREATE SCHEMA IF NOT EXISTS \" + schemaName)\n\n    if (dropIfExists) {\n      sqlContext.sql(\"DROP TABLE IF EXISTS \" + schemaName + \".\" + tableName)\n      sqlContext.sql(\"DROP TABLE IF EXISTS \" + schemaName + \".\" + tableName + \"_prqt\")\n    }\n\n    println(\"Creating table \" + tableName)\n    sqlContext.sql(\"CREATE TABLE \" + schemaName + \".\" + tableName + \" stored as sequencefile as select * from \" + tableName + \"_temp\")\n\n    println(\"Creating table \" + tableName + \"_prqt\")\n    sqlContext.sql(\"CREATE TABLE \" + schemaName + \".\" + tableName + \"_prqt stored as parquet as select * from es.\" + tableName)\n\n    sqlContext.dropTempTable(tableName + \"_temp\")\n}\n\n// Read csv to table\ndef getCsvIntoTable(fileFullPath: String, schemaName: String, tableName: String, dropIfExists: Boolean) {\n\n      var csvFile = sqlContext.read\n            .format(\"com.databricks.spark.csv\")\n            .option(\"header\", \"true\") // Use first line of all files as header\n            .option(\"inferSchema\", \"true\") // Automatically infer data types\n            .load(fileFullPath)\n      csvFile.registerTempTable(tableName + \"_temp\")\n\n      sqlContext.sql(\"CREATE SCHEMA IF NOT EXISTS \" + schemaName)\n      \n      if (dropIfExists) {\n          sqlContext.sql(\"DROP TABLE IF EXISTS \" + schemaName + \".\" + tableName)\n          sqlContext.sql(\"DROP TABLE IF EXISTS \" + schemaName + \".\" + tableName + \"_prqt\")\n      }\n      \n      println(\"Creating table \" + tableName)\n      sqlContext.sql(\"CREATE TABLE \" + schemaName + \".\" + tableName + \" stored as sequencefile as select * from \" + tableName + \"_temp\")\n      \n      println(\"Creating table \" + tableName + \"_prqt\")\n      sqlContext.sql(\"CREATE TABLE \" + schemaName + \".\" + tableName + \"_prqt stored as parquet as select * from \" + tableName + \"_temp\")\n      \n      sqlContext.dropTempTable(tableName + \"_temp\")\n}\n"},{"config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1467660044546_1922449602","id":"20160704-192044_2012549397","dateCreated":"Jul 4, 2016 7:20:44 PM","status":"READY","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:458","dateUpdated":"Jul 4, 2016 7:26:10 PM","text":""}],"name":"Example Note","id":"2BS7MNSJE","angularObjects":{"2BE97ZHKT":[],"2BFTPSDFF":[],"2BH8BP239":[],"2BDMAYNUU":[],"2BHHPN55J":[],"2BHHPDDZ2":[],"2BEEP7TWJ":[],"2BF5DXJEF":[],"2BFQRNZ7G":[],"2BGGAQ3X9":[],"2BG5VH99W":[],"2BH7XF1R9":[],"2BFP3A4ZT":[],"2BDZ52U37":[]},"config":{"looknfeel":"default"},"info":{}}